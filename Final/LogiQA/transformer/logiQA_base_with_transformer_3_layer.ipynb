{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4191e3a933dc473bb7dff1a28d8f9045": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1f23a415d6e44ab1810a4bf6402a3c0a",
              "IPY_MODEL_a52bf6ecfe904e5eb56dbaefa26054a2",
              "IPY_MODEL_cfb0371a5253463896dfb3c55f64b644"
            ],
            "layout": "IPY_MODEL_b8304d5f7696449fa05fef9da713dc78"
          }
        },
        "1f23a415d6e44ab1810a4bf6402a3c0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be215fee9b544e31954b9890b722d2a6",
            "placeholder": "​",
            "style": "IPY_MODEL_d436db7b88d2457d8e994a430938e405",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "a52bf6ecfe904e5eb56dbaefa26054a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cced00d239f74f7986d848f4c3a5b60e",
            "max": 481,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fb44f5fbdca24a60990a5874263bc5d6",
            "value": 481
          }
        },
        "cfb0371a5253463896dfb3c55f64b644": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f96547211f2443b9a2a96075ebe8dd0b",
            "placeholder": "​",
            "style": "IPY_MODEL_ab32b8de76924bfeae31e1b1291a2b56",
            "value": " 481/481 [00:00&lt;00:00, 28.0kB/s]"
          }
        },
        "b8304d5f7696449fa05fef9da713dc78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be215fee9b544e31954b9890b722d2a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d436db7b88d2457d8e994a430938e405": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cced00d239f74f7986d848f4c3a5b60e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb44f5fbdca24a60990a5874263bc5d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f96547211f2443b9a2a96075ebe8dd0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab32b8de76924bfeae31e1b1291a2b56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3973af04b6c145b7b386993d46dc30aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ee342e120dde4052b848154ca264318a",
              "IPY_MODEL_f4ac8b926977499490c2c25ac35388d8",
              "IPY_MODEL_4a982650f9624a6c9397fce16f3113a4"
            ],
            "layout": "IPY_MODEL_279bf55fc48c4f7b9fd0e88398a7e51e"
          }
        },
        "ee342e120dde4052b848154ca264318a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1ef110be94047fe9f23fe09b2f313a0",
            "placeholder": "​",
            "style": "IPY_MODEL_44fe1e9d604546c0809720961e97d383",
            "value": "Downloading (…)olve/main/vocab.json: 100%"
          }
        },
        "f4ac8b926977499490c2c25ac35388d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fddc41aa89224878beb8ff3c3eee81ea",
            "max": 898823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_73cad0924cb4454b966571efc1621e4c",
            "value": 898823
          }
        },
        "4a982650f9624a6c9397fce16f3113a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e73c60344eb943b08e08e94dfb732f24",
            "placeholder": "​",
            "style": "IPY_MODEL_48be394e835f48c7a633cb16ed76ab43",
            "value": " 899k/899k [00:01&lt;00:00, 804kB/s]"
          }
        },
        "279bf55fc48c4f7b9fd0e88398a7e51e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1ef110be94047fe9f23fe09b2f313a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44fe1e9d604546c0809720961e97d383": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fddc41aa89224878beb8ff3c3eee81ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73cad0924cb4454b966571efc1621e4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e73c60344eb943b08e08e94dfb732f24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48be394e835f48c7a633cb16ed76ab43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0642f5d312ff4060b8a8077d77c281cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_155fd2285ef04821b110f701a3aead68",
              "IPY_MODEL_fe91ac42593e44d5819949ba75b6c89f",
              "IPY_MODEL_8bb13b6c1ee647fab6cfef6bc3b606ee"
            ],
            "layout": "IPY_MODEL_f2b47ff548584707a5c27f25566727d4"
          }
        },
        "155fd2285ef04821b110f701a3aead68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8c4cfd27d38477c85dae34710a92b97",
            "placeholder": "​",
            "style": "IPY_MODEL_b2789cc169a7451c90ee4c03ada9e0b0",
            "value": "Downloading (…)olve/main/merges.txt: 100%"
          }
        },
        "fe91ac42593e44d5819949ba75b6c89f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34e31fed24e5405b9260f011e1510bfd",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3876267f334f4498ac89c0f0eb4b5e7e",
            "value": 456318
          }
        },
        "8bb13b6c1ee647fab6cfef6bc3b606ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_043b6243bd044080879420e3b8a75921",
            "placeholder": "​",
            "style": "IPY_MODEL_d4b278e4c1d84f02a94455a662a07929",
            "value": " 456k/456k [00:00&lt;00:00, 507kB/s]"
          }
        },
        "f2b47ff548584707a5c27f25566727d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8c4cfd27d38477c85dae34710a92b97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2789cc169a7451c90ee4c03ada9e0b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34e31fed24e5405b9260f011e1510bfd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3876267f334f4498ac89c0f0eb4b5e7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "043b6243bd044080879420e3b8a75921": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4b278e4c1d84f02a94455a662a07929": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "845ef5674b7e4d7bbef217d4d9c92de8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_adb4e0c9ff3242d6bf45404f0f9caf67",
              "IPY_MODEL_830fae26d4f84d42b74a1e4aff50fd9a",
              "IPY_MODEL_65e3556fde124e91a4c6de1c2c73cab9"
            ],
            "layout": "IPY_MODEL_4801fd90be8c422e891e1324f7bd7aab"
          }
        },
        "adb4e0c9ff3242d6bf45404f0f9caf67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a78a5cf55284c0ab69f43e6d6869361",
            "placeholder": "​",
            "style": "IPY_MODEL_254ce48534bf4f10a6d058e665a93df8",
            "value": "Downloading (…)/main/tokenizer.json: 100%"
          }
        },
        "830fae26d4f84d42b74a1e4aff50fd9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec2849d7eb4441659b5c73765ea135a1",
            "max": 1355863,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c5844cc436f7451c88a0cf76980d999e",
            "value": 1355863
          }
        },
        "65e3556fde124e91a4c6de1c2c73cab9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db420cc0f94b4c5394aa54213c2da288",
            "placeholder": "​",
            "style": "IPY_MODEL_a1139e370cbd4e0bb2464a9e41f81043",
            "value": " 1.36M/1.36M [00:01&lt;00:00, 1.19MB/s]"
          }
        },
        "4801fd90be8c422e891e1324f7bd7aab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a78a5cf55284c0ab69f43e6d6869361": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "254ce48534bf4f10a6d058e665a93df8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec2849d7eb4441659b5c73765ea135a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5844cc436f7451c88a0cf76980d999e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "db420cc0f94b4c5394aa54213c2da288": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1139e370cbd4e0bb2464a9e41f81043": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "14bdfefabc5347359683a73112dd5c11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8a10a2a83afa40da944d2b98df574928",
              "IPY_MODEL_182e238d62174d1a84b978453bf48ed1",
              "IPY_MODEL_44cfcfe2c1e443c48502b78e18b44d59"
            ],
            "layout": "IPY_MODEL_c63fe2b83a0f4c19b6afa5083797f189"
          }
        },
        "8a10a2a83afa40da944d2b98df574928": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3309e45f4fa404db89e4c2f9f198c27",
            "placeholder": "​",
            "style": "IPY_MODEL_f61f177d931b4cca8b59b45d39240062",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "182e238d62174d1a84b978453bf48ed1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9def7f33cec64f44b6448386d047625a",
            "max": 501200538,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_629bd8e2ea18464d88fec3d2bf6a7515",
            "value": 501200538
          }
        },
        "44cfcfe2c1e443c48502b78e18b44d59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_520ccf11dd944dbf89c2cdab2c6b8068",
            "placeholder": "​",
            "style": "IPY_MODEL_65ed9290cf904cedbea3a0fe054fcd28",
            "value": " 501M/501M [00:01&lt;00:00, 387MB/s]"
          }
        },
        "c63fe2b83a0f4c19b6afa5083797f189": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3309e45f4fa404db89e4c2f9f198c27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f61f177d931b4cca8b59b45d39240062": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9def7f33cec64f44b6448386d047625a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "629bd8e2ea18464d88fec3d2bf6a7515": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "520ccf11dd944dbf89c2cdab2c6b8068": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65ed9290cf904cedbea3a0fe054fcd28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Config"
      ],
      "metadata": {
        "id": "UvvX_L7eUTUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class config:\n",
        "    \n",
        "    device = None\n",
        "    seed = 27\n",
        "    num_workers = 2\n",
        "    prefetch_factor = 2\n",
        "    fp16 = True\n",
        "    warm_up = 0.1\n",
        "    weight_decay = 0.01\n",
        "    train_batch_size = 1\n",
        "    eval_batch_size = 1\n",
        "    train_epochs = 10\n",
        "    gradient_accumulation_steps = 6\n",
        "    adam_epsilon= 1e-6\n",
        "    adam_betas = (0.9, 0.98)\n",
        "    learning_rate= 1e-5\n",
        "    max_grad_norm=0.0\n",
        "    writer=False\n",
        "    save_steps=773\n",
        "    logging_steps=100\n",
        "    max_step=1000000\n",
        "\n",
        "    max_seq_length = 256\n",
        "    load_examples_num_workers = 2\n",
        "\n",
        "    # pretrained path\n",
        "    pretrained_model_name_or_path = 'roberta-base'\n",
        "    pretrained_model_name_or_path_cache = 'pretrained'\n",
        "\n",
        "    # local paths\n",
        "    train_data_path = '/content/dataset/train.txt'\n",
        "    val_data_path = '/content/dataset/val.txt'\n",
        "    test_data_path = '/content/dataset/test.txt'\n",
        "    output_path = 'content/output'\n",
        "    tensor_cache_path = 'content/tensor/'"
      ],
      "metadata": {
        "id": "3lmZffJ8US0i"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Installs and Imports"
      ],
      "metadata": {
        "id": "Ed9suqZvTXaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "TgkVL4oPVmEC",
        "outputId": "ddb37113-c1ee-4bec-f4c6-ffeeb311de0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkgpEycrTTUu",
        "outputId": "a4f67523-3ebe-45ef-d11b-cc015f589be1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.2-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !git lfs install\n",
        "# !git clone https://huggingface.co/roberta-large\n",
        "# !git clone https://huggingface.co/roberta-large"
      ],
      "metadata": {
        "id": "mJ6cpkJRTcDT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fvnUykmFTesL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "!wget https://www.dropbox.com/s/7q0eaosyd5zu5st/logiqa_dataset.zip \n",
        "# Full path of\n",
        "# the archive file\n",
        "filename = \"/content/logiqa_dataset.zip\"\n",
        " \n",
        "# Target directory\n",
        "extract_dir = \"/content/dataset\"\n",
        " \n",
        "# Format of archive file\n",
        "archive_format = \"zip\"\n",
        " \n",
        "# Unpack the archive file\n",
        "shutil.unpack_archive(filename, extract_dir, archive_format)\n",
        "print(\"Archive file unpacked successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-1aAXDxTgLc",
        "outputId": "55e1c5e4-b9ac-48c3-fe7b-648cfd05e2d7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-21 03:51:25--  https://www.dropbox.com/s/7q0eaosyd5zu5st/logiqa_dataset.zip\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.69.18, 2620:100:6031:18::a27d:5112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.69.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/7q0eaosyd5zu5st/logiqa_dataset.zip [following]\n",
            "--2023-03-21 03:51:26--  https://www.dropbox.com/s/raw/7q0eaosyd5zu5st/logiqa_dataset.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc1032368855b58ac802f5d40f70.dl.dropboxusercontent.com/cd/0/inline/B4pTkyP_Zw51-Lys2KEdc_NgcM_1767U9edTX9zQcbtQP8DrXA3DD7UUA38hZP6KhuqoTqbjVIxsaDt4_9jSzpo-jLHumQtbILHDlR8_nOI5T7vfy58i1XKe9hHhq_PtrAO_YEi1rLNJFdfyrml_35J6YT7Ht7ARwpJ6kTIuh3aqPA/file# [following]\n",
            "--2023-03-21 03:51:27--  https://uc1032368855b58ac802f5d40f70.dl.dropboxusercontent.com/cd/0/inline/B4pTkyP_Zw51-Lys2KEdc_NgcM_1767U9edTX9zQcbtQP8DrXA3DD7UUA38hZP6KhuqoTqbjVIxsaDt4_9jSzpo-jLHumQtbILHDlR8_nOI5T7vfy58i1XKe9hHhq_PtrAO_YEi1rLNJFdfyrml_35J6YT7Ht7ARwpJ6kTIuh3aqPA/file\n",
            "Resolving uc1032368855b58ac802f5d40f70.dl.dropboxusercontent.com (uc1032368855b58ac802f5d40f70.dl.dropboxusercontent.com)... 162.125.7.15, 2620:100:6031:15::a27d:510f\n",
            "Connecting to uc1032368855b58ac802f5d40f70.dl.dropboxusercontent.com (uc1032368855b58ac802f5d40f70.dl.dropboxusercontent.com)|162.125.7.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/B4rm04XJjIMjMH1JrMxJbH6qTF7IApvKOHCYxikQRA20sqQ2QEKuK-KI5r5Pg6s1zmcnS7O-6G_nT6c3H8-dHXq71c9xYO72O8gp9-OBNzdgJmR8t4gGkSU5c4els5RANGZt1OkZKPYbqh2cYAcvjcYaIL6msorCFJV82H6fcQRPD7zc1AFk2xqQaOvsKiYlYCQzJd8TJAbDx1JbBzj4hFqlazo7AwLaWd50HbbJ1tjcr9dCM0s-eCawFIYUTOiXKfpkFdggRIFNnxfk3igtG8PE3NA5NZtNEHHfKudzjJUvmxYH3h1iZpJ6HM_kkghIAyWzjnr90yz4SE9tfwmzI8CB2-FWWqkc-3GAOsHa21dnta4k4ECB8yKdygCB5VMBWWQ5cXqDzNGpO1I-AMYzaPcZmRHxtANDqBFkPLmO7y2O5A/file [following]\n",
            "--2023-03-21 03:51:29--  https://uc1032368855b58ac802f5d40f70.dl.dropboxusercontent.com/cd/0/inline2/B4rm04XJjIMjMH1JrMxJbH6qTF7IApvKOHCYxikQRA20sqQ2QEKuK-KI5r5Pg6s1zmcnS7O-6G_nT6c3H8-dHXq71c9xYO72O8gp9-OBNzdgJmR8t4gGkSU5c4els5RANGZt1OkZKPYbqh2cYAcvjcYaIL6msorCFJV82H6fcQRPD7zc1AFk2xqQaOvsKiYlYCQzJd8TJAbDx1JbBzj4hFqlazo7AwLaWd50HbbJ1tjcr9dCM0s-eCawFIYUTOiXKfpkFdggRIFNnxfk3igtG8PE3NA5NZtNEHHfKudzjJUvmxYH3h1iZpJ6HM_kkghIAyWzjnr90yz4SE9tfwmzI8CB2-FWWqkc-3GAOsHa21dnta4k4ECB8yKdygCB5VMBWWQ5cXqDzNGpO1I-AMYzaPcZmRHxtANDqBFkPLmO7y2O5A/file\n",
            "Reusing existing connection to uc1032368855b58ac802f5d40f70.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2492441 (2.4M) [application/zip]\n",
            "Saving to: ‘logiqa_dataset.zip’\n",
            "\n",
            "logiqa_dataset.zip  100%[===================>]   2.38M  1.99MB/s    in 1.2s    \n",
            "\n",
            "2023-03-21 03:51:30 (1.99 MB/s) - ‘logiqa_dataset.zip’ saved [2492441/2492441]\n",
            "\n",
            "Archive file unpacked successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import logging\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from torch import nn, Tensor\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data import DataLoader, RandomSampler, TensorDataset, SequentialSampler\n",
        "from torch.cuda.amp import GradScaler\n",
        "from transformers import AutoTokenizer, get_linear_schedule_with_warmup, AdamW, PreTrainedTokenizer\n",
        "from transformers.modeling_outputs import MultipleChoiceModelOutput\n",
        "from transformers.tokenization_utils_base import PaddingStrategy, TruncationStrategy\n",
        "from transformers.models.roberta.modeling_roberta import RobertaModel, RobertaPreTrainedModel, RobertaConfig, RobertaLMHead\n",
        "\n",
        "from collections import Counter\n",
        "from functools import partial\n",
        "from multiprocessing import Pool\n",
        "from typing import Dict, List\n",
        "from nltk import sent_tokenize\n",
        "from tqdm import tqdm\n",
        "from abc import ABC\n",
        "\n",
        "if config.writer:\n",
        "    from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)"
      ],
      "metadata": {
        "id": "Ymo5YM2fUhXb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Dependencies"
      ],
      "metadata": {
        "id": "bxRC3l2cUX60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(logits, labels):\n",
        "    assert logits.size()[:-1] == labels.size()\n",
        "\n",
        "    _, pred = logits.max(dim=-1)\n",
        "    true_label_num = (labels != -1).sum().item()\n",
        "    correct = (pred == labels).sum().item()\n",
        "    if true_label_num == 0:\n",
        "        return 0, 0\n",
        "    acc = correct * 1.0 / true_label_num\n",
        "    return acc, true_label_num\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        if isinstance(val, torch.Tensor):\n",
        "            val = val.item()\n",
        "        if isinstance(n, torch.Tensor):\n",
        "            n = n.item()\n",
        "\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        if self.count > 0:\n",
        "            self.avg = self.sum / self.count\n",
        "        else:\n",
        "            self.avg = 0\n",
        "\n",
        "    def save(self):\n",
        "        return {\n",
        "            'val': self.val,\n",
        "            'avg': self.avg,\n",
        "            'sum': self.sum,\n",
        "            'count': self.count\n",
        "        }\n",
        "\n",
        "    def load(self, value: dict):\n",
        "        if value is None:\n",
        "            self.reset()\n",
        "        self.val = value['val'] if 'val' in value else 0\n",
        "        self.avg = value['avg'] if 'avg' in value else 0\n",
        "        self.sum = value['sum'] if 'sum' in value else 0\n",
        "        self.count = value['count'] if 'count' in value else 0\n",
        "        \n",
        "class LogMetric(object):\n",
        "    \"\"\"\n",
        "    Record all metrics for logging.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *metric_names):\n",
        "\n",
        "        self.metrics = {\n",
        "            key: AverageMeter() for key in metric_names\n",
        "        }\n",
        "\n",
        "    def update(self, metric_name, val, n=1):\n",
        "\n",
        "        self.metrics[metric_name].update(val, n)\n",
        "\n",
        "    def reset(self, metric_name=None):\n",
        "        if metric_name is None:\n",
        "            for key in self.metrics.keys():\n",
        "                self.metrics[key].reset()\n",
        "            return\n",
        "\n",
        "        self.metrics[metric_name].reset()\n",
        "\n",
        "    def get_log(self):\n",
        "\n",
        "        log = {\n",
        "            key: self.metrics[key].avg for key in self.metrics\n",
        "        }\n",
        "        return log\n",
        "\n",
        "class LogMixin:\n",
        "    eval_metrics: LogMetric = None\n",
        "\n",
        "    def init_metric(self, *metric_names):\n",
        "        self.eval_metrics = LogMetric(*metric_names)\n",
        "\n",
        "    def get_eval_log(self, reset=False):\n",
        "        if self.eval_metrics is None:\n",
        "            print(\"The `eval_metrics` attribute hasn't been initialized.\")\n",
        "\n",
        "        results = self.eval_metrics.get_log()\n",
        "\n",
        "        _eval_metric_log = '\\t'.join([f\"{k}: {v}\" for k, v in results.items()])\n",
        "\n",
        "        if reset:\n",
        "            self.eval_metrics.reset()\n",
        "\n",
        "        return _eval_metric_log, results"
      ],
      "metadata": {
        "id": "Gc_pCNOnU7y1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model"
      ],
      "metadata": {
        "id": "mVFy-69SVGA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RobertaForMultipleChoice(RobertaPreTrainedModel, LogMixin, ABC):\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    def __init__(self, config: RobertaConfig,\n",
        "                 re_init_cls: bool = False,\n",
        "                 fs_checkpoint: bool = False,\n",
        "                 fs_checkpoint_offload_to_cpu: bool = False,\n",
        "                 fs_checkpoint_maintain_forward_counter: bool = False,\n",
        "                 freeze_encoder: bool = False,\n",
        "                 no_pooler: bool = False):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.roberta = RobertaModel(config)\n",
        "        self.lm_head = RobertaLMHead(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.re_init_cls = re_init_cls\n",
        "        if self.re_init_cls:\n",
        "            self.classifier_i = nn.Linear(config.hidden_size, 1)\n",
        "        self.classifier = nn.Linear(config.hidden_size, 1)\n",
        "        self.no_pooler = no_pooler\n",
        "        self.freeze_encoder = freeze_encoder\n",
        "        print(self.freeze_encoder)\n",
        "        if freeze_encoder:\n",
        "            for param in self.roberta.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        self.init_metric(\"loss\", \"acc\")\n",
        "        self.transformer = torch.nn.TransformerEncoder(\n",
        "            torch.nn.TransformerEncoderLayer(d_model=config.hidden_size, nhead=8),num_layers=3)\n",
        "\n",
        "    @staticmethod\n",
        "    def fold_tensor(x: Tensor):\n",
        "        if x is None:\n",
        "            return x\n",
        "        return x.reshape(-1, x.size(-1))\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            input_ids: Tensor,\n",
        "            attention_mask: Tensor = None,\n",
        "            token_type_ids: Tensor = None,\n",
        "            labels: Tensor = None,\n",
        "            sentence_index: Tensor = None,\n",
        "            sentence_mask: Tensor = None,\n",
        "            sent_token_mask: Tensor = None,\n",
        "            mlm_labels: Tensor = None,\n",
        "            output_attentions=None,\n",
        "            output_hidden_states=None,\n",
        "            return_dict=None,\n",
        "    ):\n",
        "\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "        num_choices = input_ids.shape[1]\n",
        "\n",
        "        input_ids = self.fold_tensor(input_ids)\n",
        "        attention_mask = self.fold_tensor(attention_mask)\n",
        "        token_type_ids = self.fold_tensor(token_type_ids)\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            token_type_ids=token_type_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        if self.no_pooler:\n",
        "            pooled_output = outputs[0][:, 0]\n",
        "        else:\n",
        "            pooled_output = outputs[1]\n",
        "        \n",
        "        pooled_output = self.transformer(pooled_output)\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        if self.re_init_cls:\n",
        "            logits = self.classifier_i(pooled_output)\n",
        "        else:\n",
        "            logits = self.classifier(pooled_output)\n",
        "        reshaped_logits = logits.view(-1, num_choices)\n",
        "\n",
        "        choice_mask = (attention_mask.sum(dim=-1) == 0).reshape(-1, num_choices)\n",
        "        reshaped_logits = reshaped_logits + choice_mask * -10000.0\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=-1)\n",
        "            loss = loss_fct(reshaped_logits, labels)\n",
        "\n",
        "            if mlm_labels is not None:\n",
        "                mlm_scores = self.lm_head(outputs[0])\n",
        "                mlm_loss = loss_fct(mlm_scores.reshape(-1, self.config.vocab_size), mlm_labels.reshape(-1))\n",
        "                loss += mlm_loss\n",
        "\n",
        "            if not self.training:\n",
        "                acc, true_label_num = get_accuracy(reshaped_logits, labels)\n",
        "                self.eval_metrics.update(\"acc\", val=acc, n=true_label_num)\n",
        "                self.eval_metrics.update(\"loss\", val=loss.item(), n=true_label_num)\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (reshaped_logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return MultipleChoiceModelOutput(\n",
        "            loss=loss,\n",
        "            logits=reshaped_logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )"
      ],
      "metadata": {
        "id": "VtdoNpSCTmKz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Providers"
      ],
      "metadata": {
        "id": "M0Kk_TYYVCNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sep_tokens(_tokenizer):\n",
        "    return [_tokenizer.sep_token] * (_tokenizer.max_len_single_sentence - _tokenizer.max_len_sentences_pair)\n",
        "\n",
        "\n",
        "def is_bpe(_tokenizer: PreTrainedTokenizer):\n",
        "    return _tokenizer.__class__.__name__ in [\n",
        "        \"RobertaTokenizer\",\n",
        "        \"LongformerTokenizer\",\n",
        "        \"BartTokenizer\",\n",
        "        \"RobertaTokenizerFast\",\n",
        "        \"LongformerTokenizerFast\",\n",
        "        \"BartTokenizerFast\",\n",
        "    ]\n",
        "\n",
        "\n",
        "def load_dataset(config, tokenizer, split='train'):\n",
        "    if split == 'train':\n",
        "        file_path = config.train_data_path\n",
        "    elif split == 'val':\n",
        "        file_path = config.val_data_path\n",
        "    elif split == 'test':\n",
        "        file_path = config.test_data_path\n",
        "    else:\n",
        "        raise Exception(split)\n",
        "\n",
        "    examples, features, tensors = convert_examples_into_features(file_path=file_path,\n",
        "                                                                 tokenizer=tokenizer,\n",
        "                                                                 max_seq_length=config.max_seq_length,\n",
        "                                                                 num_workers=config.load_examples_num_workers,\n",
        "                                                                suffix=split)\n",
        "    dataset = TensorDataset(*tensors)\n",
        "    return dataset, features\n",
        "\n",
        "\n",
        "def collator(batch):\n",
        "    if len(batch[0]) == 5:\n",
        "        input_ids, attention_mask, token_type_ids, labels, sentence_spans = list(zip(*batch))\n",
        "    elif len(batch[0]) == 4:\n",
        "        input_ids, attention_mask, labels, sentence_spans = list(zip(*batch))\n",
        "        token_type_ids = None\n",
        "    else:\n",
        "        raise RuntimeError()\n",
        "\n",
        "    input_ids = torch.stack(input_ids, dim=0)\n",
        "    attention_mask = torch.stack(attention_mask, dim=0)\n",
        "    labels = torch.stack(labels, dim=0)\n",
        "    sentence_spans = torch.stack(sentence_spans, dim=0)\n",
        "\n",
        "    batch, option_num, _, _ = sentence_spans.size()\n",
        "    # [batch, option_num, max_sent_num]\n",
        "    max_sent_len = (sentence_spans[:, :, :, 1] - sentence_spans[:, :, :, 0]).max().item()\n",
        "    # [batch, option_num, max_sent_num]\n",
        "    sent_mask = (sentence_spans[:, :, :, 0] != -1)\n",
        "    # [batch, option_num]\n",
        "    sent_num = sent_mask.sum(dim=2)\n",
        "    b_max_sent_num = sent_num.max().item()\n",
        "    sentence_spans = sentence_spans[:, :, :b_max_sent_num]\n",
        "    sent_mask = sent_mask[:, :, :b_max_sent_num]\n",
        "\n",
        "    sentence_index = torch.zeros(batch, option_num, b_max_sent_num, max_sent_len, dtype=torch.long)\n",
        "    sent_token_mask = torch.zeros(batch, option_num, b_max_sent_num, max_sent_len, dtype=torch.long)\n",
        "    for b_id, b_spans in enumerate(sentence_spans):\n",
        "        for op_id, op_spans in enumerate(b_spans):\n",
        "            for sent_id, span in enumerate(op_spans):\n",
        "                s, e = span[0].item(), span[1].item()\n",
        "                if s == -1:\n",
        "                    break\n",
        "                _len = e - s\n",
        "                sentence_index[b_id, op_id, sent_id, :_len] = torch.arange(s, e, dtype=torch.long)\n",
        "                sent_token_mask[b_id, op_id, sent_id, :_len] = 1\n",
        "\n",
        "    outputs = {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels,\n",
        "        \"sentence_index\": sentence_index,\n",
        "        \"sentence_mask\": sent_mask,\n",
        "        \"sent_token_mask\": sent_token_mask\n",
        "    }\n",
        "    if token_type_ids is not None:\n",
        "        outputs[\"token_type_ids\"] = torch.stack(token_type_ids, dim=0)\n",
        "\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def read_examples(file_path: str):\n",
        "    LOGIQA_LABEL_TO_ID = {\"a\": 0, \"b\": 1, \"c\": 2, \"d\": 3}\n",
        "    LOGIQA_ID_TO_LABEL = {0: \"a\", 1: \"b\", 2: \"c\", 3: \"d\"}\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as out:\n",
        "            data = out.read().split(\"\\n\\n\")\n",
        "    contexts_list = []\n",
        "    questions_list = []\n",
        "    answers_list = []\n",
        "    labels_list = []\n",
        "    examples = []\n",
        "\n",
        "    for i in data:\n",
        "        tmp_splited_i = i.split(\"\\n\")\n",
        "        contexts_list.append(tmp_splited_i[1])\n",
        "        questions_list.append(tmp_splited_i[2])\n",
        "        answers_list.append(\n",
        "            [tmp_splited_i[3], tmp_splited_i[4], tmp_splited_i[5], tmp_splited_i[6]]\n",
        "        )\n",
        "        labels_list.append(LOGIQA_LABEL_TO_ID[tmp_splited_i[0]])\n",
        "\n",
        "\n",
        "    for i in range(len(contexts_list)):\n",
        "        examples.append({\"context\": contexts_list[i],\n",
        "                    \"question\": questions_list[i],\n",
        "                    \"options\": answers_list[i],\n",
        "                    \"label\": labels_list[i]})\n",
        "  \n",
        "\n",
        "\n",
        "    print(f\"{len(examples)} examples are loaded from {file_path}.\")\n",
        "    return examples\n",
        "\n",
        "\n",
        "def _convert_example_to_features(example, tokenizer, max_seq_length):\n",
        "    context = example[\"context\"]\n",
        "    question = example[\"question\"]\n",
        "    context_sentences = [sent for sent in sent_tokenize(context) if sent]\n",
        "\n",
        "    context_tokens = []\n",
        "    for _sent_id, _sent in enumerate(context_sentences):\n",
        "        _sent_tokens = tokenizer.tokenize(_sent)\n",
        "        context_tokens.extend([(_sent_id, _tok) for _tok in _sent_tokens])\n",
        "\n",
        "    _q_sent_id_offset = len(context_sentences)\n",
        "    question_tokens = [(_q_sent_id_offset, _tok) for _tok in tokenizer.tokenize(question)]\n",
        "\n",
        "    features = []\n",
        "    for option in example[\"options\"]:\n",
        "        sep_tokens = get_sep_tokens(tokenizer)\n",
        "        _op_sent_id_offset = _q_sent_id_offset + 1\n",
        "        opt_tokens = [(_op_sent_id_offset, _tok) for _tok in tokenizer.tokenize(option)]\n",
        "\n",
        "        lens_to_remove = len(context_tokens) + len(question_tokens) + len(opt_tokens) + len(sep_tokens) + (\n",
        "                tokenizer.model_max_length - tokenizer.max_len_sentences_pair) - max_seq_length\n",
        "\n",
        "        tru_c_tokens, tru_q_o_tokens, _ = tokenizer.truncate_sequences(context_tokens,\n",
        "                                                                       question_tokens + sep_tokens + opt_tokens,\n",
        "                                                                       num_tokens_to_remove=lens_to_remove,\n",
        "                                                                       truncation_strategy=TruncationStrategy.LONGEST_FIRST)\n",
        "\n",
        "        c_tokens, q_op_tokens = [], []\n",
        "        sent_id_map = Counter()\n",
        "\n",
        "        for _sent_id, _tok in tru_c_tokens:\n",
        "            sent_id_map[_sent_id] += 1\n",
        "            c_tokens.append(_tok)\n",
        "\n",
        "        for _tok in tru_q_o_tokens:\n",
        "            if isinstance(_tok, tuple):\n",
        "                _sent_id, _tok = _tok\n",
        "                q_op_tokens.append(_tok)\n",
        "                sent_id_map[_sent_id] += 1\n",
        "            elif isinstance(_tok, str):\n",
        "                q_op_tokens.append(_tok)\n",
        "            else:\n",
        "                raise RuntimeError(_tok)\n",
        "\n",
        "        sent_span_offset = 1\n",
        "        sent_spans = []\n",
        "        for i in range(len(context_sentences) + 2):\n",
        "            if i == _q_sent_id_offset or i == _op_sent_id_offset:\n",
        "                sent_span_offset += (tokenizer.max_len_single_sentence - tokenizer.max_len_sentences_pair)\n",
        "            if i in sent_id_map:\n",
        "                _cur_len = sent_id_map.pop(i)\n",
        "                sent_spans.append((sent_span_offset, sent_span_offset + _cur_len))\n",
        "                sent_span_offset += _cur_len\n",
        "        assert not sent_id_map\n",
        "\n",
        "        tokenizer_outputs = tokenizer(tokenizer.convert_tokens_to_string(c_tokens),\n",
        "                                      text_pair=tokenizer.convert_tokens_to_string(q_op_tokens),\n",
        "                                      padding=PaddingStrategy.MAX_LENGTH,\n",
        "                                      max_length=max_seq_length)\n",
        "        assert len(tokenizer_outputs[\"input_ids\"]) == max_seq_length, (\n",
        "        len(c_tokens), len(q_op_tokens), len(tokenizer_outputs[\"input_ids\"]))\n",
        "        features.append({\n",
        "            \"input_ids\": tokenizer_outputs[\"input_ids\"],\n",
        "            \"attention_mask\": tokenizer_outputs[\"attention_mask\"],\n",
        "            \"token_type_ids\": tokenizer_outputs[\"token_type_ids\"] if \"token_type_ids\" in tokenizer_outputs else None,\n",
        "            \"sentence_spans\": sent_spans,\n",
        "        })\n",
        "\n",
        "    return {\n",
        "        \"features\": features,\n",
        "        \"label\": example[\"label\"]\n",
        "    }\n",
        "\n",
        "\n",
        "def _data_to_tensors(features):\n",
        "    data_num = len(features)\n",
        "    option_num = len(features[0][\"features\"])\n",
        "\n",
        "    input_ids = torch.tensor([[op[\"input_ids\"] for op in f[\"features\"]] for f in features])\n",
        "    attention_mask = torch.tensor([[op[\"attention_mask\"] for op in f[\"features\"]] for f in features], dtype=torch.long)\n",
        "    if features[0][\"features\"][0][\"token_type_ids\"] is not None:\n",
        "        token_type_ids = torch.tensor([[op[\"token_type_ids\"] for op in f[\"features\"]] for f in features],\n",
        "                                      dtype=torch.long)\n",
        "    else:\n",
        "        token_type_ids = None\n",
        "    labels = torch.tensor([f[\"label\"] for f in features], dtype=torch.long)\n",
        "\n",
        "    # List[List[List[Tuple[int, int]]]]\n",
        "    sentence_spans_ls = [[op[\"sentence_spans\"] for op in f[\"features\"]] for f in features]\n",
        "    max_sent_num = 0\n",
        "    for f in sentence_spans_ls:\n",
        "        f_max_sent_num = max(map(len, f))\n",
        "        max_sent_num = max(f_max_sent_num, max_sent_num)\n",
        "\n",
        "    sentence_spans = torch.zeros(data_num, option_num, max_sent_num, 2, dtype=torch.long).fill_(-1)\n",
        "    for f_id, f in enumerate(sentence_spans_ls):\n",
        "        for op_id, op in enumerate(f):\n",
        "            f_op_sent_num = len(op)\n",
        "            sentence_spans[f_id, op_id, :f_op_sent_num] = torch.tensor(op, dtype=torch.long)\n",
        "\n",
        "    if token_type_ids is not None:\n",
        "        return input_ids, attention_mask, token_type_ids, labels, sentence_spans\n",
        "    else:\n",
        "        return input_ids, attention_mask, labels, sentence_spans\n",
        "\n",
        "\n",
        "def convert_examples_into_features(file_path, tokenizer, max_seq_length, num_workers = 16, suffix=''):\n",
        "    tokenizer_name = tokenizer.__class__.__name__\n",
        "    tokenizer_name = tokenizer_name.replace('TokenizerFast', '')\n",
        "    tokenizer_name = tokenizer_name.replace('Tokenizer', '').lower()\n",
        "\n",
        "    file_suffix = f\"{tokenizer_name}_{max_seq_length}_{suffix}\"\n",
        "    cached_file_path = config.tensor_cache_path + file_suffix\n",
        "\n",
        "    if os.path.exists(cached_file_path):\n",
        "        print(f\"Loading cached file from {cached_file_path}\")\n",
        "        examples, features, tensors = torch.load(cached_file_path)\n",
        "        return examples, features, tensors\n",
        "\n",
        "    examples = read_examples(file_path)\n",
        "\n",
        "    with Pool(num_workers) as p:\n",
        "        _annotate = partial(_convert_example_to_features, tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
        "        features = list(tqdm(\n",
        "            p.imap(_annotate, examples, chunksize=32),\n",
        "            total=len(examples),\n",
        "            desc='converting examples to features:'\n",
        "        ))\n",
        "\n",
        "    print(\"Transform features into tensors...\")\n",
        "    tensors = _data_to_tensors(features)\n",
        "\n",
        "    print(f\"Saving processed features into {cached_file_path}.\")\n",
        "    if not os.path.exists(config.tensor_cache_path):\n",
        "        os.makedirs(config.tensor_cache_path)\n",
        "    torch.save((examples, features, tensors), cached_file_path)\n",
        "\n",
        "    return examples, features, tensors"
      ],
      "metadata": {
        "id": "ldBid4qYT116"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_and_backward(model, inputs, config, scaler):\n",
        "    if config.fp16 and scaler:\n",
        "        with torch.cuda.amp.autocast():\n",
        "            outputs = model(**inputs)\n",
        "            loss = outputs[\"loss\"]\n",
        "    else:\n",
        "        outputs = model(**inputs)\n",
        "        loss = outputs[\"loss\"]\n",
        "\n",
        "    if config.gradient_accumulation_steps > 1:\n",
        "        loss = loss / config.gradient_accumulation_steps\n",
        "\n",
        "    if scaler:\n",
        "        scaler.scale(loss).backward()\n",
        "    else:\n",
        "        loss.backward()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "def batch_to_device(batch, device):\n",
        "    batch_on_device = {}\n",
        "    for k, v in batch.items():\n",
        "        batch_on_device[k] = v.to(device)\n",
        "    return batch_on_device\n",
        "\n",
        "def train_model(config, train_dataset,val_dataset, model, tokenizer, start_global_step):\n",
        "    output_path_split = config.output_path.split('/')\n",
        "    log_dir = '/'.join([output_path_split[0], 'runs'] + output_path_split[1:])\n",
        "    if config.writer:\n",
        "        writer = SummaryWriter(log_dir=log_dir)\n",
        "    else:\n",
        "        writer = None\n",
        "\n",
        "    train_loader = DataLoader(dataset=train_dataset,\n",
        "                              sampler=RandomSampler(train_dataset),\n",
        "                              batch_size=config.train_batch_size,\n",
        "                              collate_fn=collator,\n",
        "                              num_workers=config.num_workers,\n",
        "                              pin_memory=True,\n",
        "                              prefetch_factor=config.prefetch_factor)\n",
        "    \n",
        "    val_loader = DataLoader(dataset=val_dataset,\n",
        "                              sampler=RandomSampler(val_dataset),\n",
        "                              batch_size=config.train_batch_size,\n",
        "                              collate_fn=collator,\n",
        "                              num_workers=config.num_workers,\n",
        "                              pin_memory=True,\n",
        "                              prefetch_factor=config.prefetch_factor)\n",
        "\n",
        "    no_decay = ['bias', 'LayerNorm.weight', 'layer_norm.weight']\n",
        "    grouped_parameters = [\n",
        "        {\n",
        "            'params': [p for n, p in model.named_parameters() if\n",
        "                       (not any(nd in n for nd in no_decay)) and p.requires_grad],\n",
        "            'weight_decay': config.weight_decay\n",
        "        },\n",
        "        {\n",
        "            'params': [p for n, p in model.named_parameters() if (any(nd in n for nd in no_decay)) and p.requires_grad],\n",
        "            'weight_decay': 0.0\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    total_steps = len(train_loader) // config.gradient_accumulation_steps * config.train_epochs\n",
        "\n",
        "    optimizer = AdamW(grouped_parameters,\n",
        "                      lr=config.learning_rate,\n",
        "                      eps=config.adam_epsilon,\n",
        "                      betas=config.adam_betas)\n",
        "    import transformers\n",
        "    scheduler = transformers.get_scheduler(\n",
        "                        \"linear\",    # Create a schedule with a learning rate that decreases linearly \n",
        "                                     # from the initial learning rate set in the optimizer to 0.\n",
        "                        optimizer = optimizer,\n",
        "                        num_warmup_steps = 0,\n",
        "                        num_training_steps = total_steps)\n",
        "    # (optimizer,\n",
        "    #                                             num_warmup_steps=int(total_steps * config.warm_up),\n",
        "    #                                             num_training_steps=total_steps)\n",
        "\n",
        "    if config.fp16 and config.device.type == 'cuda':\n",
        "        scaler = GradScaler()\n",
        "    else:\n",
        "        scaler = None\n",
        "    print(optimizer)\n",
        "    print(\"-- Start Training --\")\n",
        "    print(\"  Num examples = \", len(train_dataset))\n",
        "    print(\"  Num Epochs = \", config.train_epochs)\n",
        "    print(\"  Batch size = \", config.train_batch_size)\n",
        "    print(\"  Gradient Accumulation steps = \", config.gradient_accumulation_steps)\n",
        "    print(\"  Total optimization steps = \", total_steps)\n",
        "\n",
        "    global_step = 0\n",
        "    tr_loss, logging_loss = 0.0, 0.0\n",
        "    model.zero_grad()\n",
        "    random.seed(config.seed)\n",
        "    np.random.seed(config.seed)\n",
        "    torch.manual_seed(config.seed)\n",
        "    train_loss_array = []\n",
        "    val_loss_array = []\n",
        "    val_acc_array = []\n",
        "    for epoch in range(config.train_epochs):\n",
        "        for step, batch in enumerate(train_loader):\n",
        "\n",
        "            if global_step < start_global_step:\n",
        "                if (step + 1) % config.gradient_accumulation_steps == 0:\n",
        "                    scheduler.step()\n",
        "                    global_step += 1\n",
        "                continue\n",
        "                \n",
        "            model.train()\n",
        "            batch = batch_to_device(batch, config.device)\n",
        "            \n",
        "            loss = forward_and_backward(model, batch, config, scaler)\n",
        "            tr_loss += loss\n",
        "\n",
        "            if (step + 1) % config.gradient_accumulation_steps == 0:\n",
        "\n",
        "                if scaler:\n",
        "                    scaler.unscale_(optimizer)\n",
        "\n",
        "                if config.max_grad_norm:\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
        "\n",
        "                if scaler:\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                else:\n",
        "                    optimizer.step()\n",
        "\n",
        "                scheduler.step()\n",
        "                model.zero_grad(set_to_none=True)\n",
        "                global_step += 1\n",
        "\n",
        "                \n",
        "                train_loss_array.append((tr_loss - logging_loss))\n",
        "                if config.logging_steps > 0 and global_step % config.logging_steps == 0:\n",
        "                    if config.writer:\n",
        "                        writer.add_scalar('lr', scheduler.get_lr()[0], global_step)\n",
        "                        writer.add_scalar('loss', (tr_loss - logging_loss) / config.logging_steps, global_step)\n",
        "                    else:\n",
        "                        print('gb_step={}, loss={}'.format(global_step, (tr_loss - logging_loss) / config.logging_steps))\n",
        "                    logging_loss = tr_loss\n",
        "\n",
        "                if config.save_steps > 0 and global_step % config.save_steps == 0:\n",
        "                    output_dir = os.path.join(config.output_path, 'checkpoint-{}'.format(global_step))\n",
        "                    if not os.path.exists(output_dir):\n",
        "                        os.makedirs(output_dir)\n",
        "                        \n",
        "                    model.save_pretrained(output_dir)\n",
        "                    tokenizer.save_pretrained(output_dir)\n",
        "                    print(\"Saving model checkpoint to \", output_dir)\n",
        "                    \n",
        "            if global_step >= config.max_step:\n",
        "                break\n",
        "        \n",
        "        if global_step >= config.max_step:\n",
        "            break\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_acc = 0.0\n",
        "        num_val_steps = 0\n",
        "        correct_predictions = 0.0\n",
        "        pred_list = []\n",
        "        prob_list = []  \n",
        "        \n",
        "        for val_batch in (val_loader):\n",
        "            batch = batch_to_device(val_batch, config.device)\n",
        "            with torch.cuda.amp.autocast():\n",
        "              with torch.no_grad():\n",
        "                  outputs = model(**batch)\n",
        "                  val_loss += outputs.loss.item()\n",
        "                  num_val_steps = num_val_steps +1\n",
        "                  predictions = outputs.logits.argmax(dim=-1)\n",
        "                  targets = batch[\"labels\"]\n",
        "                  correct_predictions += (predictions == targets).sum().item()\n",
        "        val_loss /= num_val_steps\n",
        "        val_acc = (correct_predictions / len(val_loader))\n",
        "        val_loss_array.append(val_loss)\n",
        "        val_acc_array.append(val_acc)\n",
        "        print(\"VAL ACCURACY\",val_acc)\n",
        "        print(\"val_loss\",val_loss)\n",
        "    import json\n",
        "    json.dumps(val_loss_array)\n",
        "    json.dumps(val_acc_array)\n",
        "    json.dumps(train_loss_array)\n",
        "    return global_step, tr_loss / global_step"
      ],
      "metadata": {
        "id": "maPERhQHVLzk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_main(start_global_step=0):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    config.device = device\n",
        "    \n",
        "    random.seed(config.seed)\n",
        "    np.random.seed(config.seed)\n",
        "    torch.manual_seed(config.seed)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(config.pretrained_model_name_or_path, cache_dir=config.pretrained_model_name_or_path_cache)\n",
        "    model = RobertaForMultipleChoice.from_pretrained(config.pretrained_model_name_or_path, cache_dir=config.pretrained_model_name_or_path_cache)\n",
        "\n",
        "    model.to(config.device)\n",
        "\n",
        "    train_dataset, features = load_dataset(config, tokenizer=tokenizer, split='train')\n",
        "    val_dataset, features_val = load_dataset(config, tokenizer=tokenizer, split='val')\n",
        "    \n",
        "    step, loss = train_model(config, train_dataset, val_dataset, model, tokenizer, start_global_step)\n",
        "    print('Train finished, step: {}, loss: {}'.format(step, loss))"
      ],
      "metadata": {
        "id": "lLnlFq5YVOmf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "train_main()"
      ],
      "metadata": {
        "id": "j1u7-3agVWtm",
        "outputId": "63ab39bc-9ba9-4957-fd97-2d2987a140a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4191e3a933dc473bb7dff1a28d8f9045",
            "1f23a415d6e44ab1810a4bf6402a3c0a",
            "a52bf6ecfe904e5eb56dbaefa26054a2",
            "cfb0371a5253463896dfb3c55f64b644",
            "b8304d5f7696449fa05fef9da713dc78",
            "be215fee9b544e31954b9890b722d2a6",
            "d436db7b88d2457d8e994a430938e405",
            "cced00d239f74f7986d848f4c3a5b60e",
            "fb44f5fbdca24a60990a5874263bc5d6",
            "f96547211f2443b9a2a96075ebe8dd0b",
            "ab32b8de76924bfeae31e1b1291a2b56",
            "3973af04b6c145b7b386993d46dc30aa",
            "ee342e120dde4052b848154ca264318a",
            "f4ac8b926977499490c2c25ac35388d8",
            "4a982650f9624a6c9397fce16f3113a4",
            "279bf55fc48c4f7b9fd0e88398a7e51e",
            "b1ef110be94047fe9f23fe09b2f313a0",
            "44fe1e9d604546c0809720961e97d383",
            "fddc41aa89224878beb8ff3c3eee81ea",
            "73cad0924cb4454b966571efc1621e4c",
            "e73c60344eb943b08e08e94dfb732f24",
            "48be394e835f48c7a633cb16ed76ab43",
            "0642f5d312ff4060b8a8077d77c281cc",
            "155fd2285ef04821b110f701a3aead68",
            "fe91ac42593e44d5819949ba75b6c89f",
            "8bb13b6c1ee647fab6cfef6bc3b606ee",
            "f2b47ff548584707a5c27f25566727d4",
            "b8c4cfd27d38477c85dae34710a92b97",
            "b2789cc169a7451c90ee4c03ada9e0b0",
            "34e31fed24e5405b9260f011e1510bfd",
            "3876267f334f4498ac89c0f0eb4b5e7e",
            "043b6243bd044080879420e3b8a75921",
            "d4b278e4c1d84f02a94455a662a07929",
            "845ef5674b7e4d7bbef217d4d9c92de8",
            "adb4e0c9ff3242d6bf45404f0f9caf67",
            "830fae26d4f84d42b74a1e4aff50fd9a",
            "65e3556fde124e91a4c6de1c2c73cab9",
            "4801fd90be8c422e891e1324f7bd7aab",
            "7a78a5cf55284c0ab69f43e6d6869361",
            "254ce48534bf4f10a6d058e665a93df8",
            "ec2849d7eb4441659b5c73765ea135a1",
            "c5844cc436f7451c88a0cf76980d999e",
            "db420cc0f94b4c5394aa54213c2da288",
            "a1139e370cbd4e0bb2464a9e41f81043",
            "14bdfefabc5347359683a73112dd5c11",
            "8a10a2a83afa40da944d2b98df574928",
            "182e238d62174d1a84b978453bf48ed1",
            "44cfcfe2c1e443c48502b78e18b44d59",
            "c63fe2b83a0f4c19b6afa5083797f189",
            "a3309e45f4fa404db89e4c2f9f198c27",
            "f61f177d931b4cca8b59b45d39240062",
            "9def7f33cec64f44b6448386d047625a",
            "629bd8e2ea18464d88fec3d2bf6a7515",
            "520ccf11dd944dbf89c2cdab2c6b8068",
            "65ed9290cf904cedbea3a0fe054fcd28"
          ]
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4191e3a933dc473bb7dff1a28d8f9045"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3973af04b6c145b7b386993d46dc30aa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0642f5d312ff4060b8a8077d77c281cc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "845ef5674b7e4d7bbef217d4d9c92de8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "14bdfefabc5347359683a73112dd5c11"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForMultipleChoice were not initialized from the model checkpoint at roberta-base and are newly initialized: ['transformer.layers.2.self_attn.out_proj.weight', 'transformer.layers.1.self_attn.out_proj.bias', 'transformer.layers.1.norm2.weight', 'transformer.layers.0.linear2.bias', 'transformer.layers.1.linear2.weight', 'transformer.layers.2.linear1.weight', 'transformer.layers.2.self_attn.out_proj.bias', 'classifier.bias', 'transformer.layers.1.linear2.bias', 'transformer.layers.1.linear1.bias', 'transformer.layers.0.norm1.bias', 'transformer.layers.1.linear1.weight', 'transformer.layers.0.norm2.weight', 'transformer.layers.0.linear1.weight', 'transformer.layers.1.norm1.bias', 'transformer.layers.1.self_attn.in_proj_bias', 'transformer.layers.2.self_attn.in_proj_weight', 'transformer.layers.2.linear2.bias', 'transformer.layers.1.norm2.bias', 'transformer.layers.0.self_attn.out_proj.weight', 'transformer.layers.0.norm2.bias', 'transformer.layers.2.linear1.bias', 'transformer.layers.1.self_attn.out_proj.weight', 'lm_head.decoder.bias', 'transformer.layers.0.self_attn.in_proj_bias', 'transformer.layers.0.self_attn.out_proj.bias', 'transformer.layers.0.linear2.weight', 'transformer.layers.2.norm1.bias', 'transformer.layers.0.self_attn.in_proj_weight', 'transformer.layers.1.self_attn.in_proj_weight', 'transformer.layers.2.norm2.weight', 'classifier.weight', 'transformer.layers.2.norm1.weight', 'transformer.layers.0.linear1.bias', 'transformer.layers.1.norm1.weight', 'transformer.layers.0.norm1.weight', 'transformer.layers.2.self_attn.in_proj_bias', 'transformer.layers.2.linear2.weight', 'transformer.layers.2.norm2.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7376 examples are loaded from /content/dataset/train.txt.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "converting examples to features::  23%|██▎       | 1665/7376 [00:04<00:14, 387.23it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "converting examples to features::  24%|██▍       | 1793/7376 [00:04<00:14, 375.88it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "converting examples to features::  52%|█████▏    | 3841/7376 [00:10<00:09, 390.44it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "converting examples to features::  53%|█████▎    | 3944/7376 [00:10<00:14, 232.11it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "converting examples to features::  64%|██████▍   | 4737/7376 [00:12<00:06, 377.18it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "converting examples to features::  72%|███████▏  | 5313/7376 [00:14<00:05, 358.09it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "converting examples to features::  74%|███████▍  | 5441/7376 [00:14<00:05, 368.22it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "converting examples to features::  79%|███████▉  | 5825/7376 [00:15<00:04, 383.39it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "converting examples to features::  80%|███████▉  | 5889/7376 [00:15<00:03, 376.38it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "converting examples to features::  84%|████████▍ | 6209/7376 [00:16<00:03, 356.96it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "converting examples to features::  85%|████████▌ | 6273/7376 [00:17<00:03, 350.12it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "converting examples to features::  86%|████████▌ | 6337/7376 [00:17<00:02, 356.67it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "converting examples to features::  86%|████████▋ | 6373/7376 [00:17<00:02, 344.31it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "converting examples to features::  87%|████████▋ | 6433/7376 [00:17<00:02, 319.54it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "converting examples to features::  90%|████████▉ | 6625/7376 [00:18<00:02, 348.15it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "converting examples to features::  91%|█████████ | 6689/7376 [00:18<00:01, 358.73it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "converting examples to features::  92%|█████████▏| 6753/7376 [00:18<00:01, 340.88it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "converting examples to features::  93%|█████████▎| 6881/7376 [00:18<00:01, 340.35it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "converting examples to features::  96%|█████████▌| 7073/7376 [00:19<00:00, 355.42it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "converting examples to features::  98%|█████████▊| 7265/7376 [00:19<00:00, 362.94it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "converting examples to features::  99%|█████████▉| 7302/7376 [00:19<00:00, 363.53it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "converting examples to features:: 100%|██████████| 7376/7376 [00:20<00:00, 366.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transform features into tensors...\n",
            "Saving processed features into content/tensor/roberta_256_train.\n",
            "651 examples are loaded from /content/dataset/val.txt.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "converting examples to features::  49%|████▉     | 321/651 [00:01<00:00, 356.54it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "converting examples to features::  59%|█████▉    | 385/651 [00:01<00:00, 345.91it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "converting examples to features::  69%|██████▉   | 449/651 [00:01<00:00, 337.86it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "converting examples to features::  89%|████████▊ | 577/651 [00:01<00:00, 366.00it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "converting examples to features::  94%|█████████▍| 614/651 [00:01<00:00, 363.42it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "converting examples to features:: 100%|██████████| 651/651 [00:01<00:00, 348.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transform features into tensors...\n",
            "Saving processed features into content/tensor/roberta_256_val.\n",
            "AdamW (\n",
            "Parameter Group 0\n",
            "    betas: (0.9, 0.98)\n",
            "    correct_bias: True\n",
            "    eps: 1e-06\n",
            "    initial_lr: 1e-05\n",
            "    lr: 1e-05\n",
            "    weight_decay: 0.01\n",
            "\n",
            "Parameter Group 1\n",
            "    betas: (0.9, 0.98)\n",
            "    correct_bias: True\n",
            "    eps: 1e-06\n",
            "    initial_lr: 1e-05\n",
            "    lr: 1e-05\n",
            "    weight_decay: 0.0\n",
            ")\n",
            "-- Start Training --\n",
            "  Num examples =  7376\n",
            "  Num Epochs =  10\n",
            "  Batch size =  1\n",
            "  Gradient Accumulation steps =  6\n",
            "  Total optimization steps =  12290\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gb_step=100, loss=1.4067116639018058\n",
            "gb_step=200, loss=1.430679587572813\n",
            "gb_step=300, loss=1.4015796868503094\n",
            "gb_step=400, loss=1.4052062286436557\n",
            "gb_step=500, loss=1.4101920050382615\n",
            "gb_step=600, loss=1.420998624265194\n",
            "gb_step=700, loss=1.383495652973652\n",
            "Saving model checkpoint to  content/output/checkpoint-773\n",
            "gb_step=800, loss=1.396078800112009\n",
            "gb_step=900, loss=1.3878995098173619\n",
            "gb_step=1000, loss=1.392646198272705\n",
            "gb_step=1100, loss=1.3935097867250443\n",
            "gb_step=1200, loss=1.3924884803593158\n",
            "VAL ACCURACY 0.3486943164362519\n",
            "val_loss 1.364682359263278\n",
            "gb_step=1300, loss=1.3916036494076252\n",
            "gb_step=1400, loss=1.3927361954748632\n",
            "gb_step=1500, loss=1.3851686641573906\n",
            "Saving model checkpoint to  content/output/checkpoint-1546\n",
            "gb_step=1600, loss=1.4016108285635709\n",
            "gb_step=1700, loss=1.3689689495414497\n",
            "gb_step=1800, loss=1.3908684350550176\n",
            "gb_step=1900, loss=1.3577851600199937\n",
            "gb_step=2000, loss=1.381158322915435\n",
            "gb_step=2100, loss=1.3858912508189678\n",
            "gb_step=2200, loss=1.383609909415245\n",
            "gb_step=2300, loss=1.3634248074144124\n",
            "Saving model checkpoint to  content/output/checkpoint-2319\n",
            "gb_step=2400, loss=1.3476510798931123\n",
            "VAL ACCURACY 0.3533026113671275\n",
            "val_loss 1.330795949024539\n",
            "gb_step=2500, loss=1.351280909255147\n",
            "gb_step=2600, loss=1.3314275819435715\n",
            "gb_step=2700, loss=1.306179318279028\n",
            "gb_step=2800, loss=1.3042700587958098\n",
            "gb_step=2900, loss=1.2760888358205558\n",
            "gb_step=3000, loss=1.2683676980063319\n",
            "Saving model checkpoint to  content/output/checkpoint-3092\n",
            "gb_step=3100, loss=1.2598515645414592\n",
            "gb_step=3200, loss=1.2600344522856175\n",
            "gb_step=3300, loss=1.2701771592348814\n",
            "gb_step=3400, loss=1.267465751785785\n",
            "gb_step=3500, loss=1.2844428339973093\n",
            "gb_step=3600, loss=1.284079309515655\n",
            "VAL ACCURACY 0.31797235023041476\n",
            "val_loss 1.3794047602448045\n",
            "gb_step=3700, loss=1.2563291502743958\n",
            "gb_step=3800, loss=1.1281494004698471\n",
            "Saving model checkpoint to  content/output/checkpoint-3865\n",
            "gb_step=3900, loss=1.1056664198450745\n",
            "gb_step=4000, loss=1.1276310028601437\n",
            "gb_step=4100, loss=1.0882400445919485\n",
            "gb_step=4200, loss=1.0778777740336956\n",
            "gb_step=4300, loss=1.0621723909815772\n",
            "gb_step=4400, loss=1.1276254497189075\n",
            "gb_step=4500, loss=1.0977059766463935\n",
            "gb_step=4600, loss=1.1209802803653293\n",
            "Saving model checkpoint to  content/output/checkpoint-4638\n",
            "gb_step=4700, loss=1.0785366462660022\n",
            "gb_step=4800, loss=1.1129460107337217\n",
            "gb_step=4900, loss=1.1558111298270524\n",
            "VAL ACCURACY 0.3425499231950845\n",
            "val_loss 1.4484205223139255\n",
            "gb_step=5000, loss=0.9856437649484724\n",
            "gb_step=5100, loss=0.838659247647447\n",
            "gb_step=5200, loss=0.9105582952772966\n",
            "gb_step=5300, loss=0.8525442526247935\n",
            "gb_step=5400, loss=0.8259696617769078\n",
            "Saving model checkpoint to  content/output/checkpoint-5411\n",
            "gb_step=5500, loss=0.9090545932971872\n",
            "gb_step=5600, loss=0.8813916166566196\n",
            "gb_step=5700, loss=0.8885991959657986\n",
            "gb_step=5800, loss=0.8783511354110669\n",
            "gb_step=5900, loss=0.8843146899531712\n",
            "gb_step=6000, loss=0.9307869716649293\n",
            "gb_step=6100, loss=0.8743344812197029\n",
            "VAL ACCURACY 0.3486943164362519\n",
            "val_loss 1.649911785774815\n",
            "Saving model checkpoint to  content/output/checkpoint-6184\n",
            "gb_step=6200, loss=0.7339000137287076\n",
            "gb_step=6300, loss=0.6559123711629218\n",
            "gb_step=6400, loss=0.6243598470048346\n",
            "gb_step=6500, loss=0.6567186812785621\n",
            "gb_step=6600, loss=0.6629334173833195\n",
            "gb_step=6700, loss=0.6947250235570027\n",
            "gb_step=6800, loss=0.6869113958003982\n",
            "gb_step=6900, loss=0.683798927322714\n",
            "Saving model checkpoint to  content/output/checkpoint-6957\n",
            "gb_step=7000, loss=0.7194270502299969\n",
            "gb_step=7100, loss=0.6613923698970029\n",
            "gb_step=7200, loss=0.750847748929009\n",
            "gb_step=7300, loss=0.6750033254860591\n",
            "VAL ACCURACY 0.3118279569892473\n",
            "val_loss 1.7739932317106428\n",
            "gb_step=7400, loss=0.6046095248148958\n",
            "gb_step=7500, loss=0.48357099021772226\n",
            "gb_step=7600, loss=0.5042889145388836\n",
            "gb_step=7700, loss=0.4700278411416548\n",
            "Saving model checkpoint to  content/output/checkpoint-7730\n",
            "gb_step=7800, loss=0.4387597530043058\n",
            "gb_step=7900, loss=0.46147679267047353\n",
            "gb_step=8000, loss=0.5317923495723881\n",
            "gb_step=8100, loss=0.492277569733742\n",
            "gb_step=8200, loss=0.5486163643417785\n",
            "gb_step=8300, loss=0.4730195179057955\n",
            "gb_step=8400, loss=0.49040171131395255\n",
            "gb_step=8500, loss=0.5317850644588543\n",
            "Saving model checkpoint to  content/output/checkpoint-8503\n",
            "gb_step=8600, loss=0.5015132626389641\n",
            "VAL ACCURACY 0.33947772657450076\n",
            "val_loss 1.9454147540042719\n",
            "gb_step=8700, loss=0.37279485292081516\n",
            "gb_step=8800, loss=0.4405188716981138\n",
            "gb_step=8900, loss=0.3643786709435699\n",
            "gb_step=9000, loss=0.3886985158501011\n",
            "gb_step=9100, loss=0.4331281593070162\n",
            "gb_step=9200, loss=0.3867541043291021\n",
            "Saving model checkpoint to  content/output/checkpoint-9276\n",
            "gb_step=9300, loss=0.37390937713909805\n",
            "gb_step=9400, loss=0.3807750937273158\n",
            "gb_step=9500, loss=0.4168559810873012\n",
            "gb_step=9600, loss=0.372199172610126\n",
            "gb_step=9700, loss=0.42412208150044534\n",
            "gb_step=9800, loss=0.4084377995117939\n",
            "VAL ACCURACY 0.33947772657450076\n",
            "val_loss 2.289498433715225\n",
            "gb_step=9900, loss=0.31074198533524394\n",
            "gb_step=10000, loss=0.33296997677882245\n",
            "Saving model checkpoint to  content/output/checkpoint-10049\n",
            "gb_step=10100, loss=0.28933909750956444\n",
            "gb_step=10200, loss=0.313886340172794\n",
            "gb_step=10300, loss=0.3620123537977815\n",
            "gb_step=10400, loss=0.3498648098250851\n",
            "gb_step=10500, loss=0.32889251910102757\n",
            "gb_step=10600, loss=0.3051422798486419\n",
            "gb_step=10700, loss=0.3048966382012622\n",
            "gb_step=10800, loss=0.27516590735856883\n",
            "Saving model checkpoint to  content/output/checkpoint-10822\n",
            "gb_step=10900, loss=0.3352930248240955\n",
            "gb_step=11000, loss=0.339720280778256\n",
            "VAL ACCURACY 0.3486943164362519\n",
            "val_loss 2.4532878821037607\n",
            "gb_step=11100, loss=0.32262053721253325\n",
            "gb_step=11200, loss=0.2964177779223246\n",
            "gb_step=11300, loss=0.31406836190570175\n",
            "gb_step=11400, loss=0.26352395526573674\n",
            "gb_step=11500, loss=0.28159903342981124\n",
            "Saving model checkpoint to  content/output/checkpoint-11595\n",
            "gb_step=11600, loss=0.28247458851035845\n",
            "gb_step=11700, loss=0.2677394785941033\n",
            "gb_step=11800, loss=0.25795783131534333\n",
            "gb_step=11900, loss=0.2588312470515302\n",
            "gb_step=12000, loss=0.2641525728663692\n",
            "gb_step=12100, loss=0.25825027490784125\n",
            "gb_step=12200, loss=0.28758521381743773\n",
            "VAL ACCURACY 0.34101382488479265\n",
            "val_loss 2.7508869639194984\n",
            "Train finished, step: 12290, loss: 0.8216256948915102\n",
            "CPU times: user 1h 40min 32s, sys: 5min 47s, total: 1h 46min 20s\n",
            "Wall time: 1h 43min 20s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(config, split=\"val\", checkpoint_name=''):\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    config.device = device\n",
        "    tokenizer = AutoTokenizer.from_pretrained(config.output_path + '/' + checkpoint_name)\n",
        "    model = RobertaForMultipleChoice.from_pretrained(config.output_path + '/' + checkpoint_name)\n",
        "\n",
        "    model.to(config.device)\n",
        "\n",
        "    dataset, features = load_dataset(config, tokenizer, split=split)\n",
        "    eval_sampler = SequentialSampler(dataset)\n",
        "    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=config.eval_batch_size,\n",
        "                                  collate_fn=collator)\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"***** Running evaluation {} on {} *****\".format(split, checkpoint_name))\n",
        "    print(\"  Num examples =\", len(dataset))\n",
        "    print(\"  Batch size =\", config.eval_batch_size)\n",
        "\n",
        "    model.eval()\n",
        "    pred_list = []\n",
        "    prob_list = []\n",
        "\n",
        "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\", dynamic_ncols=True):\n",
        "        batch = batch_to_device(batch, config.device)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**batch)\n",
        "                probs = outputs[\"logits\"].softmax(dim=-1).detach().float().cpu()\n",
        "                prob, pred = probs.max(dim=-1)\n",
        "                pred_list.extend(pred.tolist())\n",
        "                prob_list.extend(prob.tolist())\n",
        "  \n",
        "    metric_log, results = model.get_eval_log(reset=True)\n",
        "    print(\"****** Evaluation Results ******\")\n",
        "    print(metric_log)\n",
        "    \n",
        "    prediction_file = os.path.join(config.output_path, \"{checkpoint_name}_eval_predictions.npy\")\n",
        "    np.save(prediction_file, pred_list)\n",
        "    json.dump(prob_list, open(os.path.join(config.output_path, \"{checkpoint_name}_eval_probs.json\"), \"w\"))\n",
        "    return results"
      ],
      "metadata": {
        "id": "-dFX9rYtW2pg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(config, checkpoint_name='checkpoint-11595')"
      ],
      "metadata": {
        "id": "gVEM5Ug6LcQG",
        "outputId": "aefdedd7-2e83-4ed1-f827-38e530a5201a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "Loading cached file from content/tensor/roberta_256_val\n",
            "***** Running evaluation val on checkpoint-11595 *****\n",
            "  Num examples = 651\n",
            "  Batch size = 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 651/651 [00:15<00:00, 42.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****** Evaluation Results ******\n",
            "loss: 2.684405943690538\tacc: 0.3425499231950845\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': 2.684405943690538, 'acc': 0.3425499231950845}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os, shutil\n",
        "# def make_archive(source, destination):\n",
        "#         base = os.path.basename(destination)\n",
        "#         name = base.split('.')[0]\n",
        "#         format = base.split('.')[1]\n",
        "#         archive_from = os.path.dirname(source)\n",
        "#         archive_to = os.path.basename(source.strip(os.sep))\n",
        "#         shutil.make_archive(name, format, archive_from, archive_to)\n",
        "#         shutil.move('%s.%s'%(name,format), destination)\n",
        "\n",
        "# make_archive('/content/content', '/content/drive/MyDrive/Thesis/Reclore/13march2023_res_optimizer_change.zip')"
      ],
      "metadata": {
        "id": "wZ9xw0PFPxgv",
        "outputId": "9cb48f60-18ff-4c1a-ca09-03117aa94601",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-772ee547f38a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s.%s'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmake_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/content'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Thesis/Reclore/13march2023_res_optimizer_change.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-772ee547f38a>\u001b[0m in \u001b[0;36mmake_archive\u001b[0;34m(source, destination)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0marchive_from\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0marchive_to\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marchive_from\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marchive_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s.%s'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/shutil.py\u001b[0m in \u001b[0;36mmake_archive\u001b[0;34m(base_name, format, root_dir, base_dir, verbose, dry_run, owner, group, logger)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mroot_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/shutil.py\u001b[0m in \u001b[0;36m_make_zipfile\u001b[0;34m(base_name, base_dir, verbose, dry_run, logger)\u001b[0m\n\u001b[1;32m    988\u001b[0m                     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m                         \u001b[0mzf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m                             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"adding '%s'\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/zipfile.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, filename, arcname, compress_type, compresslevel)\u001b[0m\n\u001b[1;32m   1769\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1770\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1771\u001b[0;31m                 \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1773\u001b[0m     def writestr(self, zinfo_or_arcname, data,\n",
            "\u001b[0;32m/usr/lib/python3.9/shutil.py\u001b[0m in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mfdst_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_samefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/zipfile.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_crc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrc32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_crc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compressor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1136\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1137\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_size\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2SPmHwATQNxK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}